1. Transformer (VVIP article; read in detail) - https://jalammar.github.io/illustrated-transformer/ 
2. What is self-attention mathematically? - https://towardsdatascience.com/illustrated-self-attention-2d627e33b20a#faae
3. Word Embedding - https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca
4. (Guide to Encoder-Decoder Model and Attention Mechanism using TF 2.0) - https://medium.com/better-programming/a-guide-on-the-encoder-decoder-model-and-the-attention-mechanism-401c836e2cdb
   1. Code: https://github.com/edumunozsala/NMT-encoder-decoder-Attention
5. (Dissecting BERT (The Encoder): Part I) - 
6. Deep Learning at VU Amsterdam - https://www.youtube.com/playlist?list=PLIXJ-Sacf8u60G1TwcznBmK6rEL3gmZmV


Transformer uses self-attention to propagate information between basic units of our instances. For example, pixels in case of images and nodes in cases of graphs.
